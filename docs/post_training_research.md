# Elleci V2 - Post-Training Research

Raccolta di paper e tecniche per migliorare Elleci dopo il pre-training.

---

## ğŸ¯ ALIGNMENT & PREFERENCE LEARNING

### Iterative DPO
- **Link**: https://arxiv.org/abs/2312.11456
- **Cosa fa**: DPO iterativo con esplorazione online del policy space
- **Come migliora Elleci**: Supera DPO standard del 5-10% su allineamento. Permette di raffinare iterativamente le preferenze invece di un singolo round.
- **DifficoltÃ **: â­â­â­ (Media)

### RLAIF vs RLHF
- **Link**: https://arxiv.org/abs/2309.00267
- **Cosa fa**: Usa un LLM (anche lo stesso!) per generare preferenze invece di annotatori umani
- **Come migliora Elleci**: **Self-improvement senza costi di annotazione!** Elleci puÃ² valutare le proprie risposte e migliorarsi.
- **DifficoltÃ **: â­â­ (Facile)

### RLAIF-V (Self-Alignment)
- **Link**: https://arxiv.org/abs/2405.17220
- **Cosa fa**: Self-feedback per ridurre allucinazioni (-80%)
- **Come migliora Elleci**: Riduzione drastica delle allucinazioni senza dati esterni. Il modello impara dai propri errori.
- **DifficoltÃ **: â­â­â­ (Media)

### UNA (Unified Natural Alignment)
- **Link**: https://arxiv.org/abs/2408.15339
- **Cosa fa**: Allineamento da feedback scalare (like/dislike) invece di preferenze complesse
- **Come migliora Elleci**: Feedback piÃ¹ semplice da raccogliere dagli utenti. Basta un thumbs up/down.
- **DifficoltÃ **: â­â­ (Facile)

---

## ğŸ§  REASONING & SELF-IMPROVEMENT

### SPAG (Self-Playing Adversarial Game)
- **Link**: https://arxiv.org/abs/2404.10642
- **Cosa fa**: Due copie del modello giocano un gioco adversarial (Taboo) per migliorare il reasoning
- **Come migliora Elleci**: **Migliora reasoning senza dati esterni!** Solo self-play iterativo.
- **DifficoltÃ **: â­â­â­ (Media)

### DiffCoT (Diffusion Chain-of-Thought)
- **Link**: https://arxiv.org/abs/2601.03559
- **Cosa fa**: CoT come processo di denoising diffusion con auto-correzione
- **Come migliora Elleci**: Errori nei primi step non propagano irreversibilmente. Il modello puÃ² "tornare indietro" e correggere.
- **DifficoltÃ **: â­â­â­â­ (Difficile)

### Long CoT Survey
- **Link**: https://arxiv.org/abs/2503.09567
- **Cosa fa**: Survey completa su Long CoT: deep reasoning + exploration + reflection
- **Come migliora Elleci**: Framework teorico per implementare reasoning complesso stile o1/DeepSeek-R1.
- **DifficoltÃ **: â­â­â­â­ (Difficile, richiede architettura specifica)

---

## ğŸ”€ MODEL MERGING & CRESCITA

### Dataless Knowledge Fusion
- **Link**: https://arxiv.org/abs/2212.09849
- **Cosa fa**: Merge di modelli nello spazio dei pesi senza accesso ai dati di training
- **Come migliora Elleci**: Combina Elleci con modelli specialisti (code, math) senza retraining.
- **DifficoltÃ **: â­ (Molto facile, solo merge pesi)

### FSLoRA (Federated Sketching LoRA)
- **Link**: https://arxiv.org/abs/2501.19389
- **Cosa fa**: LoRA con sketching per adattare a risorse eterogenee
- **Come migliora Elleci**: Fine-tuning efficiente su hardware limitato.
- **DifficoltÃ **: â­â­ (Facile)

### TLI (Targeted Lexical Injection)
- **Link**: https://arxiv.org/abs/2506.15415
- **Cosa fa**: LoRA su early layers per alignment cross-lingue
- **Come migliora Elleci**: Migliora allineamento italiano-inglese (+28% similarity).
- **DifficoltÃ **: â­â­ (Facile)

---

## ğŸ“Š DATA & DISTILLATION

### LLM Synthetic Data Survey
- **Link**: https://arxiv.org/abs/2406.15126
- **Cosa fa**: Survey completa su generazione dati sintetici con LLM
- **Come migliora Elleci**: Framework per generare dati di training di qualitÃ  usando LLM piÃ¹ grandi.
- **DifficoltÃ **: â­â­ (Facile)

### Vision-Flan
- **Link**: https://arxiv.org/abs/2402.11690
- **Cosa fa**: Instruction tuning in 2 fasi: task diversi prima, GPT-4 data dopo
- **Come migliora Elleci**: Solo ~1000 sample GPT-4 servono per allineare le risposte! Task diversity > quantity.
- **DifficoltÃ **: â­â­ (Facile)

### LESS (Data Selection)
- **Link**: https://arxiv.org/abs/2402.04333
- **Cosa fa**: Seleziona i dati di training piÃ¹ informativi
- **Come migliora Elleci**: Riduce dataset necessario del 50-90% mantenendo performance.
- **DifficoltÃ **: â­â­â­ (Media)

### Corpus Distillation Framework
- **Link**: https://arxiv.org/abs/2504.19565
- **Cosa fa**: Multi-agent per estrarre Q&A da letteratura scientifica
- **Come migliora Elleci**: Genera dataset domain-specific di alta qualitÃ  automaticamente.
- **DifficoltÃ **: â­â­â­ (Media)

---

## ğŸš€ FASE 2: PRIORITÃ€ IMPLEMENTAZIONE

### Alta PrioritÃ  (Facili, Alto Impatto)

| # | Tecnica | Tempo | Impatto |
|---|---------|-------|---------|
| 1 | **RLAIF** (self-reward) | 1-2 giorni | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| 2 | **Dataless Knowledge Fusion** | 2-3 ore | ğŸ”¥ğŸ”¥ğŸ”¥ |
| 3 | **UNA** (feedback scalare) | 1 giorno | ğŸ”¥ğŸ”¥ğŸ”¥ |
| 4 | **FSLoRA/LoRA** fine-tuning | 1 giorno | ğŸ”¥ğŸ”¥ğŸ”¥ |

### Media PrioritÃ  (PiÃ¹ Complesse)

| # | Tecnica | Tempo | Impatto |
|---|---------|-------|---------|
| 5 | **SPAG** (self-play reasoning) | 3-5 giorni | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| 6 | **Iterative DPO** | 2-3 giorni | ğŸ”¥ğŸ”¥ğŸ”¥ |
| 7 | **RLAIF-V** (anti-hallucination) | 2-3 giorni | ğŸ”¥ğŸ”¥ğŸ”¥ |
| 8 | **TLI** (IT-EN alignment) | 1 giorno | ğŸ”¥ğŸ”¥ |

### Avanzate (Richiedono R&D)

| # | Tecnica | Tempo | Impatto |
|---|---------|-------|---------|
| 9 | **DiffCoT** | 1-2 settimane | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |
| 10 | **Long CoT** (o1-style) | 2-4 settimane | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ |

---

## ğŸ“‹ Roadmap Suggerita

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ELLECI V1 (Pre-training)                 â”‚
â”‚                         âœ… IN CORSO                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2A: SFT Base                        â”‚
â”‚  â€¢ OpenOrca/Alpaca-CoT instruction tuning                   â”‚
â”‚  â€¢ LoRA fine-tuning (efficiente)                           â”‚
â”‚  â€¢ Tempo: ~1-2 giorni                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2B: Self-Improvement                â”‚
â”‚  â€¢ RLAIF (self-reward, nessun annotatore)                  â”‚
â”‚  â€¢ SPAG (self-play per reasoning)                          â”‚
â”‚  â€¢ Tempo: ~3-5 giorni                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASE 2C: Specializzazione                â”‚
â”‚  â€¢ Model merging con specialisti (code, math)              â”‚
â”‚  â€¢ DiffCoT per reasoning avanzato                          â”‚
â”‚  â€¢ Tempo: ~1-2 settimane                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ELLECI V2 ğŸš€                             â”‚
â”‚  â€¢ Reasoning migliorato                                     â”‚
â”‚  â€¢ Self-improvement continuo                                â”‚
â”‚  â€¢ Allucinazioni ridotte                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Documento generato: 2026-01-13*
