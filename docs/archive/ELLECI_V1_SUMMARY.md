# ğŸ¦ Elleci V1 - Project Summary

## Overview
**Elleci V1** Ã¨ un LLM italiano da 1.5B parametri con architettura ibrida innovativa.

---

## ğŸ—ï¸ Architettura

| Componente | Dettagli |
|------------|----------|
| **Dimensione** | 2048d x 24 layers (~1.5B params) |
| **Attenzione** | MLA (Multi-head Latent Attention) |
| **Sequenza** | Mamba (State Space Model) |
| **Quantizzazione** | BitNet 1.58-bit |
| **Context** | 1024 tokens |
| **Vocab** | 32,128 tokens (custom BPE EN+IT) |

---

## âš¡ Ottimizzazioni Training (Implementate)

### VelocitÃ 
| Tecnica | Speedup | Status |
|---------|---------|--------|
| Seq Curriculum (256â†’512â†’1024) | 17x early | âœ… |
| WSD Scheduler | 2.9x loss | âœ… |
| 8-bit AdamW | Memoria -25% | âœ… |
| Gradient Checkpointing | Memoria -40% | âœ… |
| CUDA Optimizations | 3-4x | âœ… |

### Convergenza
| Tecnica | Miglioramento | Status |
|---------|---------------|--------|
| LeRaC (per-layer LR) | 65% loss | âœ… |
| SWA (ultimo 20%) | Generalizzazione | âœ… |

---

## ğŸ“Š Dataset

### Phase 1 - Knowledge (90%)
- **55%** Cosmopedia V2 (English textbooks)
- **35%** Wikipedia Italiana
- **10%** Istruzioni IT (7,673 samples)

### Phase 2 - Alignment (10%)
- **20%** English maintenance
- **25%** IT Wiki maintenance
- **55%** Istruzioni IT

---

## ğŸ¯ Training Config

```python
TOTAL_STEPS = 50,000
BATCH_SIZE = 16
LEARNING_RATE = 1.5e-3
GRAD_ACCUM = 4
WARMUP = 5% (2,500 steps)
COOLDOWN = 20% (10,000 steps)  # WSD Scheduler
SWA_START = 80% (40,000 steps)
PHASE_SWITCH = 90% (45,000 steps)
```

---

## ğŸ“ File Structure

```
NanoPrime/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # NanoPrime model
â”‚   â”œâ”€â”€ config.py         # Configuration
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ mamba.py      # Mamba SSM
â”‚       â”œâ”€â”€ mla.py        # Multi-head Latent Attention
â”‚       â””â”€â”€ bitnet.py     # 1.58-bit quantization
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train_chimera.py  # Main training script
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chimera_dataset.py
â””â”€â”€ tokenizer_chimera_v2_patched/
```

---

## ğŸš€ Run Training

```bash
python scripts/train_chimera.py
```

Dry run:
```bash
python scripts/train_chimera.py --dry-run --steps 20
```

---

## â±ï¸ Tempo Stimato

| Fase | Steps | Stima |
|------|-------|-------|
| Phase 1 (Knowledge) | 45,000 | ~4-5 giorni |
| Phase 2 (Alignment) | 5,000 | ~0.5 giorni |
| **Totale** | 50,000 | **~5-6 giorni** |

---

## ğŸ”® Post-Training (Futuro)

1. **RLHF Reward Model** - Costruire reward
2. **Iterative DPO** - Alignment
3. **DiffCoT** - Reasoning fine-tuning
4. **Router Activation** - Slow/Fast path

---

*Elleci V1 - Un LLM italiano efficiente e intelligente* ğŸ‡®ğŸ‡¹
