"""
Mamba-2 - Selective State Space Model with SSD (State Space Duality)

Based on: "Transformers are SSMs: Generalized Models and Efficient Algorithms 
Through Structured State Space Duality" (arXiv:2405.21060)

Key improvements over Mamba v1:
- 2-8x faster through SSD framework
- Simplified scalar A matrix (instead of diagonal)
- Better hardware utilization on Tensor Cores
- Unified view with attention mechanisms

Authors: Tri Dao, Albert Gu (2024)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat
import math
from accelerated_scan.scalar import scan


class Mamba2Block(nn.Module):
    """
    Mamba-2 block with SSD (State Space Duality) mechanism.
    
    Key differences from Mamba v1:
    1. Scalar A (single value per head) instead of diagonal matrix
    2. Multi-head structure similar to attention
    3. Chunk-based processing for better parallelism
    4. Simplified discretization
    
    Args:
        config: MambaConfig with d_model, d_state, n_heads, etc.
    """
    def __init__(self, config):
        super().__init__()
        self.d_model = config.d_model
        self.d_state = getattr(config, 'd_state', 16)
        self.d_conv = getattr(config, 'd_conv', 4)
        self.expand = getattr(config, 'expand', 2)
        
        # Multi-head configuration (key innovation in Mamba-2)
        self.n_heads = getattr(config, 'n_heads', 8)
        self.d_inner = int(self.expand * self.d_model)
        self.head_dim = self.d_inner // self.n_heads
        
        # Chunk size for efficient processing
        self.chunk_size = getattr(config, 'chunk_size', 64)
        
        # Input projections
        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=False)
        
        # Convolution for local dependencies (same as v1)
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=self.d_conv,
            groups=self.d_inner,  # Depthwise
            padding=self.d_conv - 1,
        )
        
        # SSD parameters projections
        # Project to: dt (time step), B (input), C (output)
        self.x_proj = nn.Linear(
            self.d_inner, 
            self.n_heads + self.d_state * 2,  # dt_rank = n_heads, B and C
            bias=False
        )
        
        # Delta (dt) projection - per head
        self.dt_proj = nn.Linear(self.n_heads, self.d_inner, bias=True)
        
        # Initialize dt bias for stability (Preserve long-term memory)
        # We want small initial dt values so decay is close to 1.
        # dt_init range [0.001, 0.1] corresponds to decay [0.999, 0.9]
        dt_init_min = 0.001
        dt_init_max = 0.1
        dt_init = torch.exp(
            torch.rand(self.n_heads) * (math.log(dt_init_max) - math.log(dt_init_min))
            + math.log(dt_init_min)
        )
        # inv_softplus(x) = log(exp(x) - 1)
        # For small x, this is approx log(x)
        inv_dt = torch.log(torch.exp(dt_init) - 1)
        
        # Broadcast to d_inner (since dt_proj maps n_heads -> d_inner)
        # Each head controls head_dim channels
        inv_dt = inv_dt.repeat_interleave(self.head_dim)
        
        with torch.no_grad():
            self.dt_proj.weight.fill_(0.0) # Zero weight init for dt
            self.dt_proj.bias.copy_(inv_dt)
        
        # MAMBA-2 KEY CHANGE: Scalar A per head (not diagonal matrix)
        # This dramatically simplifies computation and enables Tensor Core usage
        # A is initialized to small negative values for stability
        self.A_log = nn.Parameter(torch.log(torch.linspace(1, 16, self.n_heads)))
        
        # D parameter (skip connection) - per head
        self.D = nn.Parameter(torch.ones(self.n_heads))
        
        # Output projection
        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)
        
        # Normalization (optional, helps stability)
        self.norm = nn.LayerNorm(self.d_inner)
        
        # Activation
        self.act = nn.SiLU()
        
    def forward(self, x, use_cache=False, past_state=None):
        """
        Forward pass through Mamba-2 block.

        Args:
            x: [batch, seq_len, d_model]
            use_cache: Whether to return state for caching (inference)
            past_state: Previous SSM state [batch, n_heads, head_dim, d_state]

        Returns:
            If use_cache:
                output: [batch, seq_len, d_model]
                present_state: [batch, n_heads, head_dim, d_state]
            Else:
                output: [batch, seq_len, d_model]
        """
        batch, seqlen, dim = x.shape

        # 1. Expand input (same as v1)
        x_expanded = self.in_proj(x)  # [batch, seq, d_inner*2]
        x_expanded, z = x_expanded.chunk(2, dim=-1)  # Split for gating

        # 2. Depthwise convolution (local context)
        x_conv = rearrange(x_expanded, 'b l d -> b d l')
        x_conv = self.conv1d(x_conv)[:, :, :seqlen]  # Trim padding
        x_conv = rearrange(x_conv, 'b d l -> b l d')
        x_conv = self.act(x_conv)

        # 3. SSD (State Space Duality) - Mamba-2 core
        if use_cache:
            y, present_state = self.ssd_forward_cached(x_conv, past_state)
        else:
            y = self.ssd_forward(x_conv)
            present_state = None

        # 4. Gate with z
        y = y * self.act(z)

        # 5. Normalization (CRITICAL for stability!)
        y = self.norm(y)

        # 6. Output projection
        output = self.out_proj(y)

        if use_cache:
            return output, present_state
        return output

    def ssd_forward_cached(self, x, past_state=None):
        """
        SSD forward pass with state caching support for inference.

        Args:
            x: [batch, seq_len, d_inner]
            past_state: Previous state [batch, n_heads, head_dim, d_state]

        Returns:
            y: [batch, seq_len, d_inner]
            present_state: [batch, n_heads, head_dim, d_state]
        """
        batch, seqlen, d_inner = x.shape

        # Get SSD parameters
        x_proj = self.x_proj(x)  # [batch, seq, n_heads + d_state*2]

        # Split projections
        dt = x_proj[..., :self.n_heads]  # [batch, seq, n_heads]
        B = x_proj[..., self.n_heads:self.n_heads+self.d_state]  # [batch, seq, d_state]
        C = x_proj[..., self.n_heads+self.d_state:]  # [batch, seq, d_state]

        # Process delta (time step)
        dt = self.dt_proj(dt)  # [batch, seq, d_inner]
        dt = F.softplus(dt)  # Ensure positive

        # Get A (SCALAR per head in Mamba-2!)
        A = -torch.exp(self.A_log.float())  # [n_heads]

        # Reshape for multi-head processing
        x_heads = rearrange(x, 'b l (h d) -> b l h d', h=self.n_heads)
        dt_heads = rearrange(dt, 'b l (h d) -> b l h d', h=self.n_heads)

        # Initialize or use past state
        if past_state is None:
            state = torch.zeros(batch, self.n_heads, self.head_dim, self.d_state,
                              device=x.device, dtype=x.dtype)
        else:
            state = past_state

        # Single-chunk processing with state
        y, final_state = self.chunk_forward(x_heads, dt_heads, None, B, C, state)

        # Reshape back
        y = rearrange(y, 'b l h d -> b l (h d)')

        # Add skip connection
        D = repeat(self.D, 'h -> 1 1 h 1').expand(batch, seqlen, self.n_heads, self.head_dim)
        D = rearrange(D, 'b l h d -> b l (h d)')
        y = y + x * D

        return y, final_state.detach()
    
    def ssd_forward(self, x):
        """
        SSD (State Space Duality) forward pass.
        
        This is the key innovation in Mamba-2:
        - Uses chunked processing for parallelism
        - Scalar A enables simpler matrix operations
        - Connects SSM to attention through structured matrices
        
        Args:
            x: [batch, seq_len, d_inner]
            
        Returns:
            y: [batch, seq_len, d_inner]
        """
        batch, seqlen, d_inner = x.shape
        
        # Get SSD parameters
        x_proj = self.x_proj(x)  # [batch, seq, n_heads + d_state*2]
        
        # Split projections
        dt = x_proj[..., :self.n_heads]  # [batch, seq, n_heads]
        B = x_proj[..., self.n_heads:self.n_heads+self.d_state]  # [batch, seq, d_state]
        C = x_proj[..., self.n_heads+self.d_state:]  # [batch, seq, d_state]
        
        # Process delta (time step)
        dt = self.dt_proj(dt)  # [batch, seq, d_inner]
        dt = F.softplus(dt)  # Ensure positive
        
        # Get A (SCALAR per head in Mamba-2!)
        A = -torch.exp(self.A_log.float())  # [n_heads]
        
        # Reshape for multi-head processing
        x_heads = rearrange(x, 'b l (h d) -> b l h d', h=self.n_heads)
        dt_heads = rearrange(dt, 'b l (h d) -> b l h d', h=self.n_heads)
        
        # Efficient chunked SSD computation
        y = self.chunked_ssd(x_heads, dt_heads, A, B, C)
        
        # Reshape back
        y = rearrange(y, 'b l h d -> b l (h d)')
        
        # Add skip connection
        D = repeat(self.D, 'h -> 1 1 h 1').expand(batch, seqlen, self.n_heads, self.head_dim)
        D = rearrange(D, 'b l h d -> b l (h d)')
        y = y + x * D
        
        return y
    
    def chunked_ssd(self, x, dt, A, B, C):
        """
        Chunked SSD computation for efficiency.
        
        Processes sequence in chunks to balance:
        - Parallel processing within chunks
        - Sequential state propagation between chunks
        
        Args:
            x: [batch, seq, n_heads, head_dim]
            dt: [batch, seq, n_heads, head_dim]
            A: [n_heads] - scalar per head
            B: [batch, seq, d_state]
            C: [batch, seq, d_state]
            
        Returns:
            y: [batch, seq, n_heads, head_dim]
        """
        batch, seqlen, n_heads, head_dim = x.shape
        
        # Pad sequence to multiple of chunk_size
        pad_len = (self.chunk_size - seqlen % self.chunk_size) % self.chunk_size
        if pad_len > 0:
            x = F.pad(x, (0, 0, 0, 0, 0, pad_len))
            dt = F.pad(dt, (0, 0, 0, 0, 0, pad_len))
            B = F.pad(B, (0, 0, 0, pad_len))
            C = F.pad(C, (0, 0, 0, pad_len))
        
        L_padded = x.shape[1]
        n_chunks = L_padded // self.chunk_size
        
        # Reshape into chunks
        x_chunks = rearrange(x, 'b (nc cs) h d -> b nc cs h d', cs=self.chunk_size)
        dt_chunks = rearrange(dt, 'b (nc cs) h d -> b nc cs h d', cs=self.chunk_size)
        B_chunks = rearrange(B, 'b (nc cs) n -> b nc cs n', cs=self.chunk_size)
        C_chunks = rearrange(C, 'b (nc cs) n -> b nc cs n', cs=self.chunk_size)
        
        # Compute decay factors: exp(A * cumsum(dt))
        # A is scalar per head, dt has per-position values
        dt_cumsum = dt_chunks.cumsum(dim=2)  # [batch, nc, cs, n_heads, head_dim]
        
        # Decay matrix: exp(A * dt_cumsum)
        # A: [n_heads] -> [1, 1, 1, n_heads, 1]
        A_expanded = A.view(1, 1, 1, n_heads, 1)
        decay = torch.exp(A_expanded * dt_cumsum)  # [batch, nc, cs, h, d]
        
        # Process each chunk (can be parallelized)
        outputs = []
        state = torch.zeros(batch, n_heads, head_dim, self.d_state, device=x.device, dtype=x.dtype)
        
        for c in range(n_chunks):
            x_c = x_chunks[:, c]  # [batch, cs, h, d]
            dt_c = dt_chunks[:, c]
            B_c = B_chunks[:, c]  # [batch, cs, d_state]
            C_c = C_chunks[:, c]
            decay_c = decay[:, c]
            
            # Within-chunk computation (parallel)
            y_c, state = self.chunk_forward(x_c, dt_c, decay_c, B_c, C_c, state)
            outputs.append(y_c)
        
        # Concatenate outputs
        y = torch.cat(outputs, dim=1)  # [batch, L_padded, h, d]
        
        # Remove padding
        if pad_len > 0:
            y = y[:, :seqlen]
        
        return y
    
    def chunk_forward(self, x, dt, decay, B, C, state):
        """
        Process a single chunk with state propagation.
        
        Args:
            x: [batch, chunk_size, n_heads, head_dim]
            dt: [batch, chunk_size, n_heads, head_dim]
            decay: [batch, chunk_size, n_heads, head_dim]
            B: [batch, chunk_size, d_state]
            C: [batch, chunk_size, d_state]
            state: [batch, n_heads, head_dim, d_state]
            
        Returns:
            y: [batch, chunk_size, n_heads, head_dim]
            new_state: [batch, n_heads, head_dim, d_state]
        """
        batch, cs, n_heads, head_dim = x.shape
        
        # Simple recurrent formulation for stability
        outputs = []
        
        for t in range(cs):
            # Get current inputs
            x_t = x[:, t]  # [batch, h, d]
            dt_t = dt[:, t]  # [batch, h, d]
            B_t = B[:, t]  # [batch, d_state]
            C_t = C[:, t]  # [batch, d_state]
            
            # Decay state: state = decay_t * state
            decay_t = decay[:, t].unsqueeze(-1)  # [batch, h, d, 1]
            state = state * decay_t
            
            # Update state: state += dt * outer(x, B)
            # x_t: [batch, h, d] -> [batch, h, d, 1]
            # B_t: [batch, d_state] -> [batch, 1, 1, d_state]
            x_t_exp = x_t.unsqueeze(-1)  # [batch, h, d, 1]
            B_t_exp = B_t.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, d_state]
            dt_t_exp = dt_t.unsqueeze(-1)  # [batch, h, d, 1]
            
            state = state + dt_t_exp * x_t_exp * B_t_exp
            
            # Output: y = sum(state * C)
            # state: [batch, h, d, d_state]
            # C_t: [batch, d_state] -> [batch, 1, 1, d_state]
            C_t_exp = C_t.unsqueeze(1).unsqueeze(1)
            y_t = (state * C_t_exp).sum(dim=-1)  # [batch, h, d]
            
            outputs.append(y_t)
        
        y = torch.stack(outputs, dim=1)  # [batch, cs, h, d]
        
        return y, state


class Mamba2BlockFast(Mamba2Block):
    """
    Optimized Mamba-2 block using matrix operations instead of loops.
    
    This version sacrifices some code clarity for speed by:
    1. Vectorizing the chunk processing
    2. Using cumulative sums efficiently
    3. Avoiding Python loops where possible
    """
    
    def chunk_forward(self, x, dt, decay, B, C, state):
        """
        Parallel scan implementation using accelerated-scan.

        Formula: h_t = decay_t * h_{t-1} + input_t
        where decay_t = exp(A * dt_t) and input_t = dt_t * x_t * B_t

        Args:
            x: [batch, chunk_size, n_heads, head_dim]
            dt: [batch, chunk_size, n_heads, head_dim]
            decay: [batch, chunk_size, n_heads, head_dim] - UNUSED (recomputed)
            B: [batch, chunk_size, d_state]
            C: [batch, chunk_size, d_state]
            state: [batch, n_heads, head_dim, d_state] - initial state

        Returns:
            y: [batch, chunk_size, n_heads, head_dim]
            new_state: [batch, n_heads, head_dim, d_state]
        """
        batch, cs, n_heads, head_dim = x.shape
        d_state = B.shape[-1]

        # 1. Compute step decay: exp(A * dt)
        A = -torch.exp(self.A_log.float())  # [n_heads]
        decay_step = torch.exp(A.view(1, 1, n_heads, 1) * dt)  # [B, cs, H, D]

        # 2. Scale input by dt
        x_scaled = dt * x  # [B, cs, H, D]

        # 3. Expand B to compute input values: x_scaled * B
        # B: [B, cs, S] -> [B, cs, H, D, S]
        B_exp = B.unsqueeze(2).unsqueeze(2).expand(-1, -1, n_heads, head_dim, -1)
        # x_scaled: [B, cs, H, D] -> [B, cs, H, D, 1]
        x_exp = x_scaled.unsqueeze(-1)
        # values: [B, cs, H, D, S]
        values = x_exp * B_exp

        # 4. Reshape for scan: [batch*heads*head_dim, d_state, chunk_size]
        # scan expects: (forget, inputs) with shape [N, C, T]
        # where N=batch dimension, C=channels, T=time
        N = batch * n_heads * head_dim

        # Reshape decay_step: [B, cs, H, D] -> [N, 1, cs] -> [N, S, cs]
        forget = decay_step.permute(0, 2, 3, 1).reshape(N, 1, cs)
        forget = forget.expand(N, d_state, cs).contiguous()

        # Reshape values: [B, cs, H, D, S] -> [N, S, cs]
        inputs = values.permute(0, 2, 3, 4, 1).reshape(N, d_state, cs).contiguous()

        # Prepend initial state to handle non-zero initial conditions
        # state: [B, H, D, S] -> [N, S, 1]
        state_flat = state.reshape(N, d_state, 1).contiguous()

        # Prepend state: forget becomes [N, S, cs+1], inputs becomes [N, S, cs+1]
        # For timestep 0: forget=1.0 (no decay), inputs=state (to inject initial state)
        forget_with_init = torch.cat([
            torch.ones_like(state_flat),  # forget_0 = 1.0
            forget
        ], dim=2)  # [N, S, cs+1]

        inputs_with_init = torch.cat([
            state_flat,  # inputs_0 = initial_state
            inputs
        ], dim=2)  # [N, S, cs+1]

        # 5. Call parallel scan: x_t = forget_t * x_{t-1} + inputs_t
        states_with_init = scan(forget_with_init, inputs_with_init)  # [N, S, cs+1]

        # Remove the prepended initial state timestep
        states = states_with_init[:, :, 1:]  # [N, S, cs]

        # 6. Reshape output states: [N, S, cs] -> [B, cs, H, D, S]
        states = states.view(batch, n_heads, head_dim, d_state, cs)
        states = states.permute(0, 4, 1, 2, 3)  # [B, cs, H, D, S]

        # 7. Compute output y = sum(state * C)
        C_exp = C.unsqueeze(2).unsqueeze(3)  # [B, cs, 1, 1, S]
        y = (states * C_exp).sum(dim=-1)  # [B, cs, H, D]

        # 8. Extract new state (last timestep)
        new_state = states[:, -1]  # [B, H, D, S]

        return y, new_state
        



class DifferentialMamba2Block(Mamba2BlockFast):
    """
    Differential Mamba-2 Block (arXiv:2507.06204)

    Adds a differential mechanism to reduce over-allocation of attention
    to irrelevant context. Key insight: model should focus more on
    changes/differences in the input rather than static context.

    The differential signal helps filter out redundant information and
    improves context utilization.

    Args:
        config: MambaConfig with d_model, d_state, n_heads, etc.
    """
    def __init__(self, config):
        super().__init__(config)

        # Differential gating mechanism
        # Projects difference signal to per-head gates
        self.diff_gate = nn.Linear(self.d_model, self.n_heads, bias=True)

        # Initialize gate bias to 0 (start with 50% gate)
        nn.init.zeros_(self.diff_gate.bias)
        nn.init.xavier_uniform_(self.diff_gate.weight, gain=0.1)

        # Optional: learnable temperature for gate sharpness
        self.gate_temperature = nn.Parameter(torch.ones(1))

    def forward(self, x, use_cache=False, past_state=None):
        """
        Forward pass with differential gating.

        Args:
            x: [batch, seq_len, d_model]
            use_cache: Whether to return state for caching (inference)
            past_state: Previous SSM state [batch, n_heads, head_dim, d_state]

        Returns:
            If use_cache:
                output: [batch, seq_len, d_model]
                present_state: [batch, n_heads, head_dim, d_state]
            Else:
                output: [batch, seq_len, d_model]
        """
        batch, seqlen, dim = x.shape

        # 1. Compute differential signal
        # diff[t] = x[t] - x[t-1]
        # First position has no previous, use zero
        x_prev = F.pad(x[:, :-1, :], (0, 0, 1, 0), value=0)  # Shift right
        diff = x - x_prev  # [batch, seq, d_model]

        # 2. Compute differential gate (per head)
        # Higher gate = more focus on change, less on static context
        gate_logits = self.diff_gate(diff)  # [batch, seq, n_heads]
        diff_gate = torch.sigmoid(gate_logits / self.gate_temperature)

        # 3. Standard Mamba-2 processing
        # Input expansion
        x_expanded = self.in_proj(x)
        x_expanded, z = x_expanded.chunk(2, dim=-1)

        # Depthwise convolution
        x_conv = rearrange(x_expanded, 'b l d -> b d l')
        x_conv = self.conv1d(x_conv)[:, :, :seqlen]
        x_conv = rearrange(x_conv, 'b d l -> b l d')
        x_conv = self.act(x_conv)

        # 4. Apply differential gating to SSM input
        # Modulate x_conv based on differential signal
        # Expand gate to match head_dim
        gate_expanded = repeat(
            diff_gate, 'b l h -> b l (h d)',
            d=self.head_dim
        )  # [batch, seq, d_inner]

        # Blend original and difference-focused processing
        # Higher gate = stronger differential focus
        x_diff = x_conv * (1 + gate_expanded * 0.5)  # Soft modulation

        # 5. SSD forward with optional caching
        if use_cache:
            y, present_state = self.ssd_forward_cached(x_diff, past_state)
        else:
            y = self.ssd_forward(x_diff)
            present_state = None

        # 6. Gate and project
        y = y * self.act(z)
        y = self.norm(y)
        output = self.out_proj(y)

        if use_cache:
            return output, present_state
        return output


class AdaptiveChunkMamba2Block(Mamba2BlockFast):
    """
    Mamba-2 Block with adaptive chunk size based on sequence length.

    Automatically selects optimal chunk size for better efficiency:
    - Short sequences (<=256): chunk_size=32
    - Medium sequences (<=512): chunk_size=64
    - Long sequences (<=1024): chunk_size=128
    - Very long sequences (>1024): chunk_size=256

    Args:
        config: MambaConfig
    """
    CHUNK_SIZES = {
        256: 32,
        512: 64,
        1024: 128,
        float('inf'): 256
    }

    def __init__(self, config):
        super().__init__(config)
        self.base_chunk_size = config.chunk_size

    def get_adaptive_chunk_size(self, seq_len: int) -> int:
        """Get optimal chunk size for sequence length."""
        for threshold, chunk_size in sorted(self.CHUNK_SIZES.items()):
            if seq_len <= threshold:
                return chunk_size
        return 256

    def forward(self, x):
        """Forward with adaptive chunk size."""
        batch, seqlen, dim = x.shape

        # Temporarily override chunk_size
        original_chunk = self.chunk_size
        self.chunk_size = self.get_adaptive_chunk_size(seqlen)

        # Standard forward
        output = super().forward(x)

        # Restore original
        self.chunk_size = original_chunk

        return output


# Compatibility wrapper to use as drop-in replacement
def create_mamba_block(config, version='v2'):
    """
    Factory function to create Mamba block.

    Args:
        config: Configuration object
        version: 'v1' for original, 'v2' for Mamba-2, 'v2_fast' for optimized,
                 'v2_diff' for Differential Mamba, 'v2_adaptive' for adaptive chunk

    Returns:
        MambaBlock instance
    """
    if version == 'v1':
        from .mamba import MambaBlock
        return MambaBlock(config)
    elif version == 'v2':
        return Mamba2Block(config)
    elif version == 'v2_fast':
        return Mamba2BlockFast(config)
    elif version == 'v2_diff':
        return DifferentialMamba2Block(config)
    elif version == 'v2_adaptive':
        return AdaptiveChunkMamba2Block(config)
    else:
        raise ValueError(f"Unknown version: {version}")


# Export
__all__ = [
    'Mamba2Block',
    'Mamba2BlockFast',
    'DifferentialMamba2Block',
    'AdaptiveChunkMamba2Block',
    'create_mamba_block'
]


if __name__ == "__main__":
    # Self-test
    print("Mamba-2 Block Self-Test")
    print("=" * 50)
    
    from dataclasses import dataclass
    
    @dataclass
    class TestConfig:
        d_model: int = 256
        d_state: int = 16
        d_conv: int = 4
        expand: int = 2
        n_heads: int = 4
        chunk_size: int = 32
    
    config = TestConfig()
    
    # Test Mamba-2
    print("\n1. Testing Mamba2Block...")
    mamba2 = Mamba2Block(config)
    x = torch.randn(2, 128, config.d_model)
    
    with torch.no_grad():
        y = mamba2(x)
    
    print(f"   Input: {x.shape}")
    print(f"   Output: {y.shape}")
    assert y.shape == x.shape, "Shape mismatch!"
    print("   ✅ Mamba2Block works!")
    
    # Test Mamba-2 Fast
    print("\n2. Testing Mamba2BlockFast...")
    mamba2_fast = Mamba2BlockFast(config)
    
    with torch.no_grad():
        y_fast = mamba2_fast(x)
    
    print(f"   Input: {x.shape}")
    print(f"   Output: {y_fast.shape}")
    assert y_fast.shape == x.shape, "Shape mismatch!"
    print("   ✅ Mamba2BlockFast works!")
    
    # Speed comparison
    if torch.cuda.is_available():
        print("\n3. Speed Comparison (CUDA)...")
        import time
        
        mamba2_cuda = Mamba2Block(config).cuda()
        mamba2_fast_cuda = Mamba2BlockFast(config).cuda()
        x_cuda = torch.randn(4, 512, config.d_model, device='cuda')
        
        # Warmup
        for _ in range(5):
            _ = mamba2_cuda(x_cuda)
            _ = mamba2_fast_cuda(x_cuda)
        
        torch.cuda.synchronize()
        
        # Benchmark standard
        start = time.time()
        for _ in range(20):
            _ = mamba2_cuda(x_cuda)
        torch.cuda.synchronize()
        time_std = time.time() - start
        
        # Benchmark fast
        start = time.time()
        for _ in range(20):
            _ = mamba2_fast_cuda(x_cuda)
        torch.cuda.synchronize()
        time_fast = time.time() - start
        
        print(f"   Mamba2Block: {time_std*1000:.2f}ms")
        print(f"   Mamba2BlockFast: {time_fast*1000:.2f}ms")
        print(f"   Speedup: {time_std/time_fast:.2f}x")
    
    print("\n" + "=" * 50)
    print("✅ All tests passed!")
